import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import joblib
import traceback
import matplotlib.pyplot as plt
import io
import base64

app = Flask(__name__, template_folder='templates')
CORS(app)

# Load models
nb_model = joblib.load('naive_bayes.pkl')
dt_model = joblib.load('decision_tree.pkl')
svm_model = joblib.load('svm_adversarial_trained.pkl')
rf_model = joblib.load('random_forest.pkl')
lstm_model = load_model('lstm_model.h5')

# Load preprocessing tools
with open('tokenizer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)
with open('vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)
with open('scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)
with open('pca.pkl', 'rb') as f:
    pca = pickle.load(f)

@app.route('/')
def home():
    return render_template('index.html')

def preprocess_email(emails, model_type):
    """Preprocess emails based on the model type."""
    email_vectors = vectorizer.transform(emails)
    if model_type == 'naive_bayes':
        return email_vectors.toarray()  # Naive Bayes expects original feature space without PCA
    email_vectors = scaler.transform(email_vectors.toarray())
    email_vectors = pca.transform(email_vectors)
    return email_vectors

def preprocess_email_lstm(emails):
    """Preprocess emails for the LSTM model."""
    sequences = tokenizer.texts_to_sequences(emails)
    padded_sequences = pad_sequences(sequences, maxlen=500)
    return padded_sequences

def generate_adversarial_examples_random(model, tokenizer, texts, epsilon=0.1):
    sequences = tokenizer.texts_to_sequences(texts)
    x_test = pad_sequences(sequences, maxlen=500)
    x_test = x_test.astype(np.float32)
    
    # Apply random perturbations
    perturbation = np.random.normal(0, epsilon, x_test.shape)
    adversarial_x_test = x_test + perturbation
    adversarial_x_test = np.clip(adversarial_x_test, 0, 1)
    
    return x_test, adversarial_x_test

def evaluate_with_adversarial_examples(model, texts, labels, epsilon=0.1):
    try:
        x_test, adversarial_examples = generate_adversarial_examples_random(model, tokenizer, texts, epsilon)
        print(f"x_test shape: {x_test.shape}")
        print(f"adversarial_examples shape: {adversarial_examples.shape}")
        
        predictions = model.predict(adversarial_examples)
        print(f"Raw predictions: {predictions}")
        
        predictions = (predictions > 0.5).astype(int)
        print(f"Binary predictions: {predictions}")
        
        accuracy = np.mean(predictions == labels)
        print(f"Accuracy: {accuracy}")
        
        return accuracy
    except Exception as e:
        print(f"Error in evaluate_with_adversarial_examples: {e}")
        traceback.print_exc()
        return None

def perturbation_attack(x, epsilon):
    """Apply small perturbations to the input features."""
    x_adv = np.copy(x).astype(np.float32)
    noise = np.random.normal(0, epsilon, x_adv.shape)
    x_adv = np.clip(x_adv + noise, 0, 1)
    return x_adv

def generate_adversarial_examples_perturbation(emails, epsilon, model_type):
    """Generate adversarial examples using perturbation attack for non-LSTM models."""
    processed_emails = preprocess_email(emails, model_type)
    adversarial_examples = perturbation_attack(processed_emails, epsilon)
    return processed_emails, adversarial_examples

def evaluate_model_with_adversarial_examples_perturbation(model, emails, labels, model_type, epsilon=0.1):
    """Evaluate non-LSTM models with adversarial examples generated by perturbation attack."""
    processed_emails, adversarial_emails = generate_adversarial_examples_perturbation(emails, epsilon, model_type)
    predictions = model.predict(adversarial_emails)
    
    # Debugging: Print shapes and contents
    print(f"Model: {model_type}")
    print("Adversarial Emails Shape:", adversarial_emails.shape)
    print("Predictions Shape:", predictions.shape)
    print("Predictions Content:", predictions)
    
    # Ensure the predictions array is handled correctly based on the model type
    if model_type == 'naive_bayes':
        predictions = predictions.flatten()  # Naive Bayes output might need to be flattened
    else:
        predictions = (predictions > 0.5).astype(np.int32).flatten()

    accuracy = sum(pred == label for pred, label in zip(predictions, labels)) / len(labels)
    
    # Debugging: Print predictions and labels
    print(f"Model: {model_type}")
    print("Predictions:", predictions)
    print("Labels:", labels)
    print("Accuracy:", accuracy)
    
    return accuracy

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        email_text = data['email']
        model_choice = data['model']

        if not email_text:
            return jsonify({'error': 'Email content cannot be empty.'}), 400

        if model_choice == 'naive_bayes':
            model = nb_model
            processed_email = preprocess_email([email_text], model_choice)
        elif model_choice == 'decision_tree':
            model = dt_model
            processed_email = preprocess_email([email_text], model_choice)
        elif model_choice == 'svm':
            model = svm_model
            processed_email = preprocess_email([email_text], model_choice)
        elif model_choice == 'random_forest':
            model = rf_model
            processed_email = preprocess_email([email_text], model_choice)
        elif model_choice == 'lstm':
            model = lstm_model
            processed_email = preprocess_email_lstm([email_text])
        else:
            return jsonify({'error': 'Invalid model choice.'}), 400

        # Logging feature dimension and type
        print(f"Processed email shape: {processed_email.shape}")
        print(f"Processed email dtype: {processed_email.dtype}")

        prediction = model.predict(processed_email)

        if model_choice == 'lstm':
            prediction = prediction[0][0]
            result = 'spam' if prediction > 0.5 else 'not spam'
        else:
            prediction = prediction[0]
            result = 'spam' if prediction == 1 else 'not spam'

        return jsonify({'prediction': result})
    except Exception as e:
        print(f"Error in /predict route: {e}")
        traceback.print_exc()  # For debugging
        return jsonify({'error': f'An error occurred. Please try again. Details: {str(e)}'}), 500

@app.route('/evaluate_adversarial', methods=['POST'])
def evaluate_adversarial():
    try:
        data = request.get_json()
        email_texts = data['emails']
        model_choice = data['model']
        labels = np.array(data['labels']).astype(np.int32)

        if model_choice == 'naive_bayes':
            model = nb_model
            accuracy = evaluate_model_with_adversarial_examples_perturbation(model, email_texts, labels, model_choice)
        elif model_choice == 'decision_tree':
            model = dt_model
            accuracy = evaluate_model_with_adversarial_examples_perturbation(model, email_texts, labels, model_choice)
        elif model_choice == 'svm':
            model = svm_model
            accuracy = evaluate_model_with_adversarial_examples_perturbation(model, email_texts, labels, model_choice)
        elif model_choice == 'random_forest':
            model = rf_model
            accuracy = evaluate_model_with_adversarial_examples_perturbation(model, email_texts, labels, model_choice)
        elif model_choice == 'lstm':
            model = lstm_model
            accuracy = evaluate_with_adversarial_examples(model, email_texts, labels)
        else:
            return jsonify({'error': 'Invalid model choice.'}), 400

        return jsonify({
            'model': model_choice,
            'adversarial_accuracy': accuracy.tolist() if isinstance(accuracy, np.ndarray) else accuracy
        })
    except Exception as e:
        print(f"Error in /evaluate_adversarial route: {e}")
        traceback.print_exc()
        return jsonify({'error': f'An error occurred. Please try again. Details: {str(e)}'}), 500

@app.route('/visualize_perturbation', methods=['POST'])
def visualize_perturbation():
    try:
        data = request.get_json()
        email_text = data['email']
        epsilon = data.get('epsilon', 0.1)
        model_choice = data['model']

        if not email_text:
            return jsonify({'error': 'Email content cannot be empty.'}), 400

        if model_choice == 'lstm':
            original, adversarial = generate_adversarial_examples_random(lstm_model, tokenizer, [email_text], epsilon)
        else:
            return jsonify({'error': 'Visualization is only available for LSTM model.'}), 400

        # Visualize the perturbations
        fig, ax = plt.subplots(3, 1, figsize=(12, 8))

        ax[0].imshow(original[0].reshape(1, -1), cmap='gray', aspect='auto')
        ax[0].set_title('Original Input')
        
        ax[1].imshow(adversarial[0].reshape(1, -1), cmap='gray', aspect='auto')
        ax[1].set_title('Adversarial Input')
        
        perturbation = adversarial[0] - original[0]
        ax[2].imshow(perturbation.reshape(1, -1), cmap='gray', aspect='auto')
        ax[2].set_title('Perturbation')

        plt.tight_layout()
        
        # Convert plot to PNG image
        buf = io.BytesIO()
        plt.savefig(buf, format='png')
        buf.seek(0)
        image_png = buf.getvalue()
        buf.close()

        # Encode PNG image to base64 string
        image_base64 = base64.b64encode(image_png).decode('utf-8')

        return jsonify({'image_base64': image_base64})
    except Exception as e:
        print(f"Error in /visualize_perturbation route: {e}")
        traceback.print_exc()  # For debugging
        return jsonify({'error': f'An error occurred. Please try again. Details: {str(e)}'}), 500

if __name__ == '__main__':
    app.run(debug=True)

